{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Data Science Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I picked label 7 because I like horses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Process\n",
    "\n",
    "To create my model I first created dataframes that were comprised of 50% images of horses and 50% images of other things. The training dataframe had 10000 images in total, while the testing dataframe had 2000 images in total. Following this I then split the dataframes into 4 variables. These 4 variables were x_train, y_train, x_test and y_test. With these variables I converted them into lists so that they can be processed and used to generate a model.\n",
    "\n",
    "The shape of my model contains 1 Conv layer with 64 filters, 1 MaxPooling layer, 1 Flatten layer, 1 Dense layer with 128 nodes and 1 Dense layer with 1 node. This information can be summed up with the model summary here:\n",
    "\n",
    "![image](image/summary.PNG)\n",
    "\n",
    "For metrics I used the accuracy metrics and I trained the model for 20 epochs with a batch size of 1000 and used the TensorBoard callback and the EarlyStopping callback.\n",
    "\n",
    "I used the Adam optimizer with a learning rate of 0.0001 alongside the BinaryCrossentropy loss function. The Adam optimizer was selected because it was an adaptive optimizer, meaning that it would compute adaptive learning rates for each parameter and use past gradients to calculate current gradients. This would make it a generally good optimization algorithm for deep learning and thus suited my purposes, hence why it was used.\n",
    "\n",
    "As for the Binary Crossentropy loss function, I used it because it would compute how close the output probability (between 0 and 1) would be to the real value which is either 0 or 1. This made it suitable for my model's pruposes and hence why I ended up using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance\n",
    "\n",
    "Accuracy (Y-Axis) against Epochs (X-Axis) for Training\n",
    "\n",
    "![image](image/accuracy_against_epoch_train.PNG)\n",
    "\n",
    "Accuracy (Y-Axis) against Epochs (X-Axis) for Validation\n",
    "\n",
    "![image](image/accuracy_against_epoch_validation.PNG)\n",
    "\n",
    "Validation Loss (Y-Axis) against Epochs (X-Axis) for Training\n",
    "\n",
    "![image](image/val_loss_against_epoch_train.PNG)\n",
    "\n",
    "Validation Loss (Y-Axis) against Epochs (X-Axis) for Validation\n",
    "\n",
    "![image](image/val_loss_against_epoch_validation.PNG)\n",
    "\n",
    "\n",
    "For both the training and validation, the model performs well as the accuracy increases with more epochs while the validation loss decreases with more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In conclusion, my generated model after fitting the training sets had a validation accuracy of ~81%. I am satisfied with this result as I managed to generate this model by only performing 20 epochs and know that with more time and epochs the model accuracy could be significantly higher. My generated model had a single Conv layer and 2 Dense layers, which means that the accuracy could potentially be improved by modifying the shape of my model by adding in more dense layers.\n",
    "\n",
    "With that being said, the model worked very well with image prediction. This was demonstrated when we fed it a resized 32x32 image of a horse and it predicted that there was a ~92.5% probability that the image did indeed include a horse. With hyper parameterization the new tuned model had a higher accuracy than the original model which is expected when we consider that both models were trained for 20 epochs/iterations, with the new model having tuned parameters. However, this difference was rather minimal and with more epochs completed we could see a bigger, more notable difference in the accuracies between the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Learning Multiple Layers of Features from Tiny Images](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "866edbe6bc078d6c764486c8065f78f1b4840c1926347f0a9fa139e5d055f612"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
